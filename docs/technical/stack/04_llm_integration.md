# LLM Integration: LiteLLM


---

# ‚ö†Ô∏è –ß–ï–†–ù–û–í–ò–ö ‚Äî –¢–†–ï–ë–£–ï–¢ –ü–†–û–í–ï–†–ö–ò ‚ö†Ô∏è

**–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –ù–ï —è–≤–ª—è–µ—Ç—Å—è —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º!**

–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑, –∫—Ä–∏—Ç–∏–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –ø—Ä–∏–Ω—è—Ç–∏–µ–º —Ä–µ—à–µ–Ω–∏–π.

---
## –†–µ—à–µ–Ω–∏–µ

**–í—ã–±—Ä–∞–Ω: LiteLLM (unified interface)**

---

## –ü–æ—á–µ–º—É LiteLLM?

### 1. –ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—Å–µ—Ö LLM

**–ü—Ä–æ–±–ª–µ–º–∞:** –ö–∞–∂–¥—ã–π LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–º–µ–µ—Ç —Å–≤–æ–π API:
- OpenAI ‚Üí `openai.ChatCompletion.create()`
- Anthropic ‚Üí `anthropic.messages.create()`
- Google ‚Üí `google.generativeai.generate_content()`
- Cohere ‚Üí `cohere.generate()`

**–†–µ—à–µ–Ω–∏–µ LiteLLM:** –û–¥–∏–Ω –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—Å–µ—Ö

```python
from litellm import completion, acompletion

# OpenAI
response = completion(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}]
)

# Anthropic (—Ç–æ—Ç –∂–µ –∫–æ–¥!)
response = completion(
    model="claude-3-opus-20240229",
    messages=[{"role": "user", "content": "Hello"}]
)

# Google (—Ç–æ—Ç –∂–µ –∫–æ–¥!)
response = completion(
    model="gemini-pro",
    messages=[{"role": "user", "content": "Hello"}]
)

# Cohere (—Ç–æ—Ç –∂–µ –∫–æ–¥!)
response = completion(
    model="command-r-plus",
    messages=[{"role": "user", "content": "Hello"}]
)
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –¥–ª—è AI_TEAM:**
- ‚úÖ –ú–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å = –º–µ–Ω—è–µ–º 1 —Å—Ç—Ä–æ–∫—É –≤ –∫–æ–Ω—Ñ–∏–≥–µ
- ‚úÖ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π = –ª–µ–≥–∫–æ
- ‚úÖ Fallback (–µ—Å–ª–∏ GPT-4 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Üí Claude) = –≤—Å—Ç—Ä–æ–µ–Ω–æ

---

### 2. Automatic Retry + Fallback

**LiteLLM –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å:**

```python
# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –∏–∑ config/llm.yaml
config = load_config("llm.yaml")

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π retry –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
response = completion(
    model=config["default_model"],
    messages=[{"role": "user", "content": "Hello"}],
    num_retries=config["num_retries"],  # –ò–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    timeout=config["timeout"]  # –ò–∑ –∫–æ–Ω—Ñ–∏–≥–∞
)

# Fallback –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å (—Å–ø–∏—Å–æ–∫ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
response = completion(
    model=config["default_model"],
    messages=[{"role": "user", "content": "Hello"}],
    fallbacks=config["fallback_models"]  # –ò–∑ –∫–æ–Ω—Ñ–∏–≥–∞
)
# –ï—Å–ª–∏ default_model –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Üí –ø–æ–ø—Ä–æ–±—É–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑ fallback_models
```

**–î–ª—è AI_TEAM —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ:**
- –ê–≥–µ–Ω—Ç—ã –Ω–µ –¥–æ–ª–∂–Ω—ã –ø–∞–¥–∞—Ç—å –∏–∑-–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º API
- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –¥–æ–ª–∂–µ–Ω –∂–¥–∞—Ç—å –µ—Å–ª–∏ –æ–¥–∏–Ω LLM –º–µ–¥–ª–µ–Ω–Ω—ã–π

---

### 3. Cost Tracking –≤—Å—Ç—Ä–æ–µ–Ω

**LiteLLM –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—á–∏—Ç–∞–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç—å:**

```python
import litellm

# –í–∫–ª—é—á–∞–µ–º tracking
litellm.success_callback = ["cost_tracking"]

response = completion(
    model="gpt-4",
    messages=[{"role": "user", "content": "Write an article"}]
)

# –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤—ã–∑–æ–≤–∞
cost = response._hidden_params.get("response_cost")
print(f"Cost: ${cost:.4f}")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º –≤—ã–∑–æ–≤–∞–º
print(litellm.cost_tracker.get_total_cost())
# {'gpt-4': 0.35, 'claude-3-opus': 0.12}
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ:**
- ‚úÖ –ó–Ω–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –∫–∞–∂–¥–∞—è –∑–∞–¥–∞—á–∞
- ‚úÖ –ú–æ–∂–µ–º –ø–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Å—Ç–æ–∏–º–æ—Å—Ç—å
- ‚úÖ –ë—é–¥–∂–µ—Ç–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å (–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –µ—Å–ª–∏ –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç)

---

### 4. Caching –∏–∑ –∫–æ—Ä–æ–±–∫–∏

**LiteLLM –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–µ—à:**

```python
import litellm

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ Redis –∏–∑ config/redis.yaml
redis_config = load_config("redis.yaml")
llm_config = load_config("llm.yaml")

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Redis –¥–ª—è –∫–µ—à–∞ (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
litellm.cache = litellm.Cache(
    type="redis",
    host=redis_config["host"],
    port=redis_config["port"],
    ttl=llm_config["cache_ttl"]  # –ò–∑ –∫–æ–Ω—Ñ–∏–≥–∞
)

# –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ ‚Äî —Ä–µ–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ API
response1 = completion(
    model=llm_config["default_model"],
    messages=[{"role": "user", "content": "What is AI?"}],
    cache={"ttl": llm_config["cache_ttl"]}
)

# –í—Ç–æ—Ä–æ–π –≤—ã–∑–æ–≤ —Å —Ç–µ–º –∂–µ –∑–∞–ø—Ä–æ—Å–æ–º ‚Äî –∏–∑ –∫–µ—à–∞ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ!)
response2 = completion(
    model=llm_config["default_model"],
    messages=[{"role": "user", "content": "What is AI?"}],
    cache={"ttl": llm_config["cache_ttl"]}
)

print(response2._hidden_params.get("cache_hit"))  # True
```

**–≠–∫–æ–Ω–æ–º–∏—è –¥–ª—è AI_TEAM:**
- ‚úÖ –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–µ —Ç—Ä–∞—Ç—è—Ç –¥–µ–Ω—å–≥–∏
- ‚úÖ –ë—ã—Å—Ç—Ä–µ–µ –æ—Ç–≤–µ—Ç—ã (–∏–∑ –∫–µ—à–∞ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ)
- ‚úÖ –ú–µ–Ω—å—à–µ –Ω–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ LLM API

---

### 5. Observability: Callbacks

**LiteLLM callbacks –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:**

```python
import litellm

def custom_callback(kwargs, completion_response, start_time, end_time):
    """–õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –≤—ã–∑–æ–≤ LLM"""
    model = kwargs["model"]
    latency = end_time - start_time
    tokens = completion_response.usage.total_tokens
    cost = completion_response._hidden_params.get("response_cost", 0)

    print(f"[LLM] {model} | {tokens} tokens | {latency:.2f}s | ${cost:.4f}")

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Prometheus/OpenTelemetry
    metrics.record_llm_call(model, tokens, latency, cost)

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º callback
litellm.success_callback = [custom_callback]

# –í—Å–µ –≤—ã–∑–æ–≤—ã –±—É–¥—É—Ç –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å—Å—è
response = completion(model="gpt-4", messages=[...])
```

**–î–ª—è AI_TEAM observability:**
- ‚úÖ –í–∏–¥–∏–º –≤—Å–µ LLM –≤—ã–∑–æ–≤—ã
- ‚úÖ –ú–æ–∂–µ–º —Ç—Ä–µ–π—Å–∏—Ç—å —á–µ—Ä–µ–∑ trace_id
- ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

---

## –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –∏ –ø–æ—á–µ–º—É –ù–ï–¢

### –ü—Ä—è–º—ã–µ SDK (OpenAI, Anthropic, etc.)

**–ü–ª—é—Å—ã:**
- –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
- –ü–æ–ª–Ω—ã–π feature set

**–ü–æ—á–µ–º—É –ù–ï–¢:**
- ‚ùå –†–∞–∑–Ω—ã–µ API –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
- ‚ùå –ù—É–∂–Ω–æ –ø–∏—Å–∞—Ç—å abstraction layer —Å–∞–º–∏–º
- ‚ùå –ù–µ—Ç unified retry/fallback
- ‚ùå –ù–µ—Ç cost tracking –∏–∑ –∫–æ—Ä–æ–±–∫–∏

**–ü—Ä–∏–º–µ—Ä –ø—Ä–æ–±–ª–µ–º—ã:**
```python
# OpenAI
from openai import OpenAI
client = OpenAI()
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}]
)

# Anthropic (–î–†–£–ì–û–ô API!)
from anthropic import Anthropic
client = Anthropic()
response = client.messages.create(
    model="claude-3-opus-20240229",
    messages=[{"role": "user", "content": "Hello"}],
    max_tokens=1024  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä!
)

# –†–∞–∑–Ω—ã–µ response formats, —Ä–∞–∑–Ω—ã–µ error handling, —Ä–∞–∑–Ω–∞—è –ª–æ–≥–∏–∫–∞
```

**–í–µ—Ä–¥–∏–∫—Ç:** –ü—Ä—è–º—ã–µ SDK ‚Äî –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å. –î–ª—è multi-model ‚Äî overkill.

---

### LangChain LLM Wrappers

**–ü–ª—é—Å—ã:**
- Unified interface
- –ú–Ω–æ–≥–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π

**–ü–æ—á–µ–º—É –ù–ï–¢:**
- ‚ùå –¢—è–∂–µ–ª–∞—è –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è (BaseLanguageModel, BaseChatModel)
- ‚ùå –°–ª–æ–∂–Ω–µ–µ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –≤—ã–∑–æ–≤–æ–≤
- ‚ùå –ù–µ—Ç cost tracking –∏–∑ –∫–æ—Ä–æ–±–∫–∏
- ‚ùå –ù–µ—Ç automatic fallback

**–ü—Ä–∏–º–µ—Ä LangChain:**
```python
from langchain.chat_models import ChatOpenAI, ChatAnthropic

# OpenAI
llm_openai = ChatOpenAI(model="gpt-4")
response = llm_openai.predict("Hello")

# Anthropic (–Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å –¥—Ä—É–≥–æ–π –æ–±—ä–µ–∫—Ç!)
llm_anthropic = ChatAnthropic(model="claude-3-opus-20240229")
response = llm_anthropic.predict("Hello")

# –°–ª–æ–∂–Ω–µ–µ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏
```

**–í–µ—Ä–¥–∏–∫—Ç:** LangChain wrappers —Ö–æ—Ä–æ—à–∏ –¥–ª—è chains/agents, –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –≤—ã–∑–æ–≤–æ–≤ ‚Äî overkill.

---

### Haystack LLM Integration

**–ü–ª—é—Å—ã:**
- Unified interface
- Document-focused

**–ü–æ—á–µ–º—É –ù–ï–¢:**
- ‚ùå –§–æ–∫—É—Å –Ω–∞ RAG/–¥–æ–∫—É–º–µ–Ω—Ç—ã (–Ω–µ –Ω–∞—à use case)
- ‚ùå –ú–µ–Ω—å—à–µ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –º–æ–¥–µ–ª–µ–π vs LiteLLM
- ‚ùå Heavier framework

**–í–µ—Ä–¥–∏–∫—Ç:** Haystack –¥–ª—è RAG —Å–∏—Å—Ç–µ–º, –Ω–µ –¥–ª—è multi-agent orchestration.

---

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### LLM Service Layer

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Agents Layer                          ‚îÇ
‚îÇ  WriterAgent | CriticAgent | EditorAgent | ...          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îÇ calls
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  LLM Service                             ‚îÇ
‚îÇ                  (LiteLLM wrapper)                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  - completion(model, messages, **kwargs)                ‚îÇ
‚îÇ  - acompletion (async version)                          ‚îÇ
‚îÇ  - retry logic                                          ‚îÇ
‚îÇ  - fallback logic                                       ‚îÇ
‚îÇ  - cost tracking                                        ‚îÇ
‚îÇ  - caching (Redis)                                      ‚îÇ
‚îÇ  - observability (callbacks)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îÇ uses
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   LiteLLM                                ‚îÇ
‚îÇ  Unified interface to all LLM providers                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                ‚îÇ                ‚îÇ
        ‚ñº                ‚ñº                ‚ñº
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ OpenAI  ‚îÇ      ‚îÇAnthropic‚îÇ      ‚îÇ Google  ‚îÇ
  ‚îÇ   API   ‚îÇ      ‚îÇ   API   ‚îÇ      ‚îÇ   API   ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –¥–ª—è AI_TEAM

### LLM Service Wrapper

```python
from typing import Optional, List, Dict, Any
from litellm import acompletion, completion
import litellm
from pydantic import BaseModel

class LLMConfig(BaseModel):
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LLM –∏–∑ config/llm.yaml

    ‚ö†Ô∏è –ë–ï–ó –î–ï–§–û–õ–¢–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô!
    –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏–∑ YAML –∫–æ–Ω—Ñ–∏–≥–∞.
    """
    default_model: str
    fallback_models: List[str]
    temperature: float
    max_tokens: int
    timeout: int
    num_retries: int
    cache_ttl: int

class LLMService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM —á–µ—Ä–µ–∑ LiteLLM"""

    def __init__(self, config: LLMConfig, redis_config: dict):
        """
        Args:
            config: LLMConfig –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –∏–∑ config/llm.yaml
            redis_config: dict –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –∏–∑ config/redis.yaml
        """
        self.config = config

        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–µ—à (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ redis_config)
        litellm.cache = litellm.Cache(
            type="redis",
            host=redis_config["host"],
            port=redis_config["port"],
            ttl=config.cache_ttl
        )

        # –í–∫–ª—é—á–∞–µ–º callbacks –¥–ª—è observability
        litellm.success_callback = [self._log_success]
        litellm.failure_callback = [self._log_failure]

    async def complete(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        trace_id: Optional[str] = None,
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ LLM.

        Returns:
            {
                "content": "..response text..",
                "model": "gpt-4",
                "tokens": 150,
                "cost": 0.0045,
                "cached": False
            }
        """
        model = model or self.config.default_model
        temperature = temperature or self.config.temperature
        max_tokens = max_tokens or self.config.max_tokens

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è observability
        metadata = {"trace_id": trace_id} if trace_id else {}

        try:
            response = await acompletion(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=self.config.timeout,
                num_retries=self.config.num_retries,
                fallbacks=self.config.fallback_models,
                cache={"ttl": self.config.cache_ttl} if use_cache else None,
                metadata=metadata
            )

            return {
                "content": response.choices[0].message.content,
                "model": response.model,
                "tokens": response.usage.total_tokens,
                "cost": response._hidden_params.get("response_cost", 0),
                "cached": response._hidden_params.get("cache_hit", False)
            }

        except Exception as e:
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
            print(f"[LLM Error] {e}")
            raise

    def _log_success(self, kwargs, completion_response, start_time, end_time):
        """Callback –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –≤—ã–∑–æ–≤–µ"""
        latency = end_time - start_time
        model = kwargs.get("model")
        tokens = completion_response.usage.total_tokens
        cost = completion_response._hidden_params.get("response_cost", 0)
        trace_id = kwargs.get("metadata", {}).get("trace_id")

        print(f"[LLM Success] trace_id={trace_id} model={model} tokens={tokens} latency={latency:.2f}s cost=${cost:.4f}")

        # TODO: Send to OpenTelemetry/Prometheus

    def _log_failure(self, kwargs, completion_response, start_time, end_time):
        """Callback –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        model = kwargs.get("model")
        error = completion_response
        trace_id = kwargs.get("metadata", {}).get("trace_id")

        print(f"[LLM Failure] trace_id={trace_id} model={model} error={error}")

        # TODO: Send to error tracking
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ Agent

```python
class WriterAgent(Agent):
    def __init__(self, config: AgentConfig, mindbus: MindBus, llm_service: LLMService):
        super().__init__(config, mindbus)
        self.llm_service = llm_service

    async def execute(self, task: dict, context: dict, trace_id: str) -> dict:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á—É –Ω–∞–ø–∏—Å–∞–Ω–∏—è"""

        messages = [
            {"role": "system", "content": f"You are a professional writer."},
            {"role": "user", "content": f"Write about: {task['topic']}"}
        ]

        # –í—ã–∑—ã–≤–∞–µ–º LLM —á–µ—Ä–µ–∑ —Å–µ—Ä–≤–∏—Å
        result = await self.llm_service.complete(
            messages=messages,
            model=self.config.llm_model,  # –ú–æ–∂–µ—Ç –±—ã—Ç—å gpt-4, claude, etc.
            temperature=self.config.temperature,
            trace_id=trace_id
        )

        return {
            "article": result["content"],
            "metadata": {
                "model": result["model"],
                "tokens": result["tokens"],
                "cost": result["cost"],
                "cached": result["cached"]
            }
        }
```

---

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π (YAML)

```yaml
# config/llm.yaml
llm:
  default_model: "gpt-4"

  fallback_models:
    - "claude-3-opus-20240229"
    - "gpt-3.5-turbo"

  temperature: 0.7
  max_tokens: 2000
  timeout: 60
  num_retries: 3
  cache_ttl: 3600

  # –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ (–º–æ–∂–µ–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å)
  agent_models:
    writer: "gpt-4"
    critic: "claude-3-opus-20240229"  # Claude –ª—É—á—à–µ –¥–ª—è –∫—Ä–∏—Ç–∏–∫–∏
    editor: "gpt-4"
    researcher: "perplexity-online"  # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è research

  # –ë—é–¥–∂–µ—Ç–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å
  budget:
    max_cost_per_task: 0.50  # $0.50 max per task
    max_cost_per_day: 10.00  # $10 max per day
```

### –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞

```python
import yaml
from pydantic import BaseModel

class LLMBudget(BaseModel):
    max_cost_per_task: float
    max_cost_per_day: float

class LLMFullConfig(BaseModel):
    default_model: str
    fallback_models: List[str]
    temperature: float
    max_tokens: int
    timeout: int
    num_retries: int
    cache_ttl: int
    agent_models: Dict[str, str]
    budget: LLMBudget

# –ó–∞–≥—Ä—É–∑–∫–∞
with open("config/llm.yaml") as f:
    config_dict = yaml.safe_load(f)
    llm_config = LLMFullConfig(**config_dict["llm"])

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
llm_service = LLMService(llm_config)
```

---

## Multi-Model Strategy

**–†–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á:**

| Agent | –ú–æ–¥–µ–ª—å | –ü–æ—á–µ–º—É |
|-------|--------|--------|
| **Writer** | GPT-4 | –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ |
| **Critic** | Claude 3 Opus | –õ—É—á—à–∞—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ |
| **Editor** | GPT-4 | –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –ø—Ä–∞–≤–∫–∞ |
| **Researcher** | Perplexity | –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π web search |
| **Fact-Checker** | GPT-4 | –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å |

```python
# –í –∫–æ–Ω—Ñ–∏–≥–µ –∞–≥–µ–Ω—Ç–∞ —É–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
agents:
  - name: "writer"
    role: "writer"
    llm_model: "gpt-4"  # ‚Üê –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç default

  - name: "critic"
    role: "critic"
    llm_model: "claude-3-opus-20240229"  # ‚Üê Claude –¥–ª—è –∫—Ä–∏—Ç–∏–∫–∏

  - name: "researcher"
    role: "researcher"
    llm_model: "perplexity-online"  # ‚Üê –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
```

---

## Cost Optimization

### 1. –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
```python
# –ü–µ—Ä–≤—ã–π —Ä–∞–∑ ‚Äî —Ä–µ–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤
result1 = await llm_service.complete(
    messages=[{"role": "user", "content": "What is AI?"}],
    use_cache=True
)
# Cost: $0.002

# –í—Ç–æ—Ä–æ–π —Ä–∞–∑ ‚Äî –∏–∑ –∫–µ—à–∞
result2 = await llm_service.complete(
    messages=[{"role": "user", "content": "What is AI?"}],
    use_cache=True
)
# Cost: $0 (–∫–µ—à!)
```

### 2. –£–º–Ω—ã–π fallback (–¥–µ—à–µ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á)
```python
llm_config = LLMConfig(
    default_model="gpt-4",  # –î–æ—Ä–æ–≥–∞—è, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è
    fallback_models=[
        "gpt-3.5-turbo",  # –ï—Å–ª–∏ gpt-4 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –¥–µ—à–µ–≤–ª–µ
    ]
)
```

### 3. Budget control
```python
class LLMService:
    async def complete(self, messages, **kwargs):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±—é–¥–∂–µ—Ç
        current_cost = get_today_cost()
        if current_cost >= self.config.budget.max_cost_per_day:
            raise BudgetExceededError("Daily budget exceeded")

        result = await acompletion(...)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫
        update_cost_tracker(result["cost"])

        return result
```

---

## –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ

**LiteLLM ‚Äî –∏–¥–µ–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –¥–ª—è AI_TEAM –ø–æ—Ç–æ–º—É —á—Ç–æ:**

1. ‚úÖ –ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤—Å–µ—Ö LLM (OpenAI, Anthropic, Google, Cohere, etc.)
2. ‚úÖ Automatic retry + fallback = –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å
3. ‚úÖ Cost tracking –∏–∑ –∫–æ—Ä–æ–±–∫–∏
4. ‚úÖ Caching = —ç–∫–æ–Ω–æ–º–∏—è –¥–µ–Ω–µ–≥ + —Å–∫–æ—Ä–æ—Å—Ç—å
5. ‚úÖ Observability —á–µ—Ä–µ–∑ callbacks
6. ‚úÖ –ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–º–∏–Ω–∏–º—É–º –∫–æ–¥–∞)
7. ‚úÖ Multi-model strategy (—Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤)

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –æ—Ç–∫–ª–æ–Ω–µ–Ω—ã:**
- –ü—Ä—è–º—ã–µ SDK ‚Äî —Ä–∞–∑–Ω—ã–µ API –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
- LangChain wrappers ‚Äî —Ç—è–∂–µ–ª–∞—è –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è
- Haystack ‚Äî —Ñ–æ–∫—É—Å –Ω–∞ RAG, –Ω–µ –Ω–∞—à use case

**–†–∏—Å–∫–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã:**
- LiteLLM battle-tested (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ production)
- Active development
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Å–µ—Ö major LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤

---

**–°—Ç–∞—Ç—É—Å:** üìù –ß–ï–†–ù–û–í–ò–ö (—Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è)
**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ:** 2025-12-13
